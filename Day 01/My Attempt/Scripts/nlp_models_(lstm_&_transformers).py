# -*- coding: utf-8 -*-
"""NLP Models (LSTM & Transformers).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJPB-ZQ6e1bDeWz2EqxlO2uASCD-F3hG
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.metrics import accuracy_score, classification_report

# Parameters
max_features = 20000  # Vocabulary size
maxlen = 200          # Maximum sequence length
embedding_dim = 128
batch_size = 64
epochs = 5

# Load IMDB dataset
print("Loading data...")
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

print(f"{len(x_train)} training samples")
print(f"{len(x_test)} testing samples")

# Pad sequences
print("Padding sequences...")
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

# Build LSTM model
print("Building model...")
model = Sequential()
model.add(Embedding(max_features, embedding_dim, input_length=maxlen))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

# Compile model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Summary
model.summary()

# Train model
print("Training model...")
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.2)

# Evaluate model
print("Evaluating model...")
y_pred = model.predict(x_test, batch_size=batch_size)
y_pred = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

print("Classification Report:")
print(classification_report(y_test, y_pred))

"""### Code Descriptor

This script is a machine learning pipeline designed to classify movie reviews from the IMDB dataset as either positive or negative using a Long Short-Term Memory (LSTM) network. Below is a detailed breakdown of its components and functionality:

---

#### **1. Imports and Libraries**
```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.metrics import accuracy_score, classification_report
```
- **TensorFlow and Keras**: Provides tools for building and training neural networks.
- **IMDB Dataset**: A dataset of movie reviews labeled as positive or negative.
- **Sequence Preprocessing**: Handles text data preparation (e.g., padding sequences).
- **Sequential Model**: A linear stack of layers for the neural network.
- **LSTM Layer**: A recurrent layer suitable for sequential data.
- **Sklearn Metrics**: Used to evaluate model performance with accuracy and classification reports.

---

#### **2. Parameters**
```python
max_features = 20000  # Vocabulary size
maxlen = 200          # Maximum sequence length
embedding_dim = 128
batch_size = 64
epochs = 5
```
- **`max_features`**: Limits the vocabulary size to the most common 20,000 words.
- **`maxlen`**: Pads or truncates sequences to a maximum length of 200.
- **`embedding_dim`**: Dimensionality of word embeddings.
- **`batch_size`**: Number of samples processed at once during training.
- **`epochs`**: Number of full passes through the training data.

---

#### **3. Load and Preprocess the Dataset**
```python
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
```
- **Loading Data**: Downloads and loads the IMDB dataset, retaining only the top `max_features` words.
- **Padding Sequences**: Ensures all input sequences have the same length (`maxlen`) by padding or truncating.

---

#### **4. Model Architecture**
```python
model = Sequential()
model.add(Embedding(max_features, embedding_dim, input_length=maxlen))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
```
- **Embedding Layer**: Converts words into dense vectors of size `embedding_dim`.
- **LSTM Layer**: Captures temporal dependencies in the sequence data with dropout for regularization.
- **Dense Layer**: A single neuron with a sigmoid activation function for binary classification.

---

#### **5. Compilation**
```python
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
```
- **Loss Function**: Binary cross-entropy is used as the task is a binary classification problem.
- **Optimizer**: Adam optimizer is used for efficient training.
- **Metrics**: Tracks accuracy during training.

---

#### **6. Model Training**
```python
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.2)
```
- **Training**: The model is trained on 80% of the training data, reserving 20% for validation.
- **Validation Split**: Helps monitor overfitting.

---

#### **7. Evaluation**
```python
y_pred = model.predict(x_test, batch_size=batch_size)
y_pred = (y_pred > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(classification_report(y_test, y_pred))
```
- **Predictions**: Model outputs probabilities, which are thresholded at 0.5 for binary predictions.
- **Metrics**: Computes accuracy and a detailed classification report, including precision, recall, and F1-score.

---

### **Key Concepts**
1. **LSTM**: Designed for sequential data like text, capable of capturing dependencies over long sequences.
2. **IMDB Dataset**: A popular dataset for natural language processing (NLP) tasks.
3. **Word Embeddings**: Dense representations of words learned during training.
4. **Binary Classification**: Outputs a prediction of either positive (1) or negative (0) sentiment.

### **Workflow Summary**
1. Load and preprocess text data.
2. Define the LSTM-based model architecture.
3. Compile and train the model on the training set.
4. Evaluate model performance using accuracy and a classification report on the test set.
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, Dropout, GlobalAveragePooling1D
from tensorflow.keras.layers import MultiHeadAttention, Add
from sklearn.metrics import accuracy_score, classification_report

# Parameters
max_features = 20000  # Vocabulary size
maxlen = 200          # Maximum sequence length
embedding_dim = 128
num_heads = 4
ff_dim = 128
dropout_rate = 0.2
batch_size = 64
epochs = 5

# Load IMDB dataset
print("Loading data...")
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

print(f"{len(x_train)} training samples")
print(f"{len(x_test)} testing samples")

# Pad sequences
print("Padding sequences...")
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

# Positional Encoding
def get_positional_encoding(maxlen, d_model):
    pos = tf.range(start=0, limit=maxlen, delta=1, dtype=tf.float32)  # Cast to float32
    i = tf.range(start=0, limit=d_model, delta=1, dtype=tf.float32)  # Cast to float32
    angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    angle_rads = tf.expand_dims(pos, 1) * angle_rates
    sines = tf.math.sin(angle_rads[:, 0::2])
    cosines = tf.math.cos(angle_rads[:, 1::2])
    pos_encoding = tf.concat([sines, cosines], axis=-1)
    pos_encoding = pos_encoding[tf.newaxis, ...]
    return pos_encoding  # Already float32

# Transformer Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Multi-Head Attention
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = Add()([x, inputs])
    x = LayerNormalization(epsilon=1e-6)(x)

    # Feed Forward
    x_ff = Dense(ff_dim, activation='relu')(x)
    x_ff = Dense(inputs.shape[-1])(x_ff)
    x_ff = Dropout(dropout)(x_ff)
    x = Add()([x_ff, x])
    x = LayerNormalization(epsilon=1e-6)(x)
    return x

# Build Transformer model
print("Building Transformer model...")
inputs = Input(shape=(maxlen,))
embedding_layer = Embedding(max_features, embedding_dim)(inputs)
pos_encoding = get_positional_encoding(maxlen, embedding_dim)
x = embedding_layer + pos_encoding[:, :maxlen, :]

# Transformer Encoder
x = transformer_encoder(x, head_size=embedding_dim, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate)
x = GlobalAveragePooling1D()(x)
x = Dropout(dropout_rate)(x)
x = Dense(20, activation='relu')(x)
x = Dropout(dropout_rate)(x)
outputs = Dense(1, activation='sigmoid')(x)

model = Model(inputs, outputs)

# Compile model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Summary
model.summary()

# Train model
print("Training model...")
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.2)

# Evaluate model
print("Evaluating model...")
y_pred = model.predict(x_test, batch_size=batch_size)
y_pred = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

print("Classification Report:")
print(classification_report(y_test, y_pred))

"""### Code Descriptor

This script implements a Transformer-based model for binary sentiment classification on the IMDB movie review dataset. The architecture uses self-attention mechanisms and feed-forward layers, combining features of the Transformer encoder for sequential text classification.

---

#### **1. Imports and Libraries**
```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, Dropout, GlobalAveragePooling1D
from tensorflow.keras.layers import MultiHeadAttention, Add
from sklearn.metrics import accuracy_score, classification_report
```
- **TensorFlow and Keras**: Provide tools for deep learning and model building.
- **IMDB Dataset**: Contains labeled movie reviews (positive/negative).
- **Layers for Transformers**: Includes `MultiHeadAttention`, `LayerNormalization`, and `Add` for self-attention and residual connections.
- **Sklearn Metrics**: For evaluating model performance.

---

#### **2. Parameters**
```python
max_features = 20000  # Vocabulary size
maxlen = 200          # Maximum sequence length
embedding_dim = 128
num_heads = 4
ff_dim = 128
dropout_rate = 0.2
batch_size = 64
epochs = 5
```
- **`max_features`**: Restricts vocabulary to the 20,000 most frequent words.
- **`maxlen`**: Ensures all sequences are padded or truncated to a length of 200.
- **`embedding_dim`**: Size of word embeddings and the Transformer model's input features.
- **`num_heads`**: Number of attention heads in the multi-head attention layer.
- **`ff_dim`**: Dimensionality of the feed-forward network.
- **`dropout_rate`**: Dropout rate for regularization.

---

#### **3. Load and Preprocess Dataset**
```python
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
```
- **Dataset**: Loads and preprocesses the IMDB movie review dataset.
- **Padding Sequences**: Ensures consistent input length by padding or truncating sequences to `maxlen`.

---

#### **4. Positional Encoding**
```python
def get_positional_encoding(maxlen, d_model):
    # Computes sinusoidal positional encodings for input sequences.
```
- **Purpose**: Adds position information to word embeddings since Transformers lack inherent sequence order.
- **Implementation**: Generates sinusoidal encodings as a matrix of shape `(maxlen, embedding_dim)`.

---

#### **5. Transformer Encoder**
```python
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Implements a single Transformer encoder block.
```
- **Components**:
  1. **Multi-Head Attention**: Learns relationships between words in a sequence.
  2. **Residual Connection & Layer Normalization**: Stabilizes training.
  3. **Feed-Forward Network**: Applies dense layers for transformation.
  4. **Dropout**: Adds regularization to prevent overfitting.

---

#### **6. Build Transformer Model**
```python
inputs = Input(shape=(maxlen,))
embedding_layer = Embedding(max_features, embedding_dim)(inputs)
pos_encoding = get_positional_encoding(maxlen, embedding_dim)
x = embedding_layer + pos_encoding[:, :maxlen, :]
x = transformer_encoder(x, head_size=embedding_dim, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate)
x = GlobalAveragePooling1D()(x)
x = Dropout(dropout_rate)(x)
x = Dense(20, activation='relu')(x)
x = Dropout(dropout_rate)(x)
outputs = Dense(1, activation='sigmoid')(x)
model = Model(inputs, outputs)
```
- **Embedding Layer**: Converts tokens to dense vectors.
- **Positional Encoding**: Adds positional information to embeddings.
- **Transformer Encoder**: Applies self-attention and feed-forward transformations.
- **GlobalAveragePooling1D**: Reduces sequence outputs to a single feature vector.
- **Dense Layers**: Includes a hidden layer and a final output layer for binary classification.

---

#### **7. Compile and Train the Model**
```python
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.2)
```
- **Binary Cross-Entropy**: Loss function for binary classification.
- **Adam Optimizer**: Adaptive learning rate for efficient training.
- **Validation Split**: Reserves 20% of training data for validation during training.

---

#### **8. Evaluate the Model**
```python
y_pred = model.predict(x_test, batch_size=batch_size)
y_pred = (y_pred > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(classification_report(y_test, y_pred))
```
- **Thresholding**: Converts probabilities to binary predictions (`1` if probability > 0.5, otherwise `0`).
- **Metrics**: Computes accuracy and a classification report with precision, recall, and F1-score.

---

### **Key Concepts**
1. **Transformer Encoder**: Uses self-attention to capture relationships between words in a sequence.
2. **Positional Encoding**: Embeds positional information into the sequence.
3. **Binary Classification**: Predicts movie review sentiment (positive/negative).
4. **Regularization**: Uses dropout layers to mitigate overfitting.

---

### **Workflow Summary**
1. Load and preprocess text data with padding and truncation.
2. Define a Transformer-based architecture with embedding, positional encoding, self-attention, and dense layers.
3. Train the model with the IMDB dataset.
4. Evaluate the model's performance on test data using accuracy and classification metrics.
"""