# -*- coding: utf-8 -*-
"""lstm-with-one-hot-for-emotion-recognition workshop version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n5TaLbGD3A3zQdXI-fXLHODCi3mJoL2o

<span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Dataset Download and Upload in the Colab </h1></span>

For downloading and importing the dataset in you colab please follow the following steps:

*   Go to https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp
*   Download the dataset. It will be downloaded as archive.zip in your pc
*   Upload the dataset by running the following code cells
"""

from google.colab import files

# Prompt user to upload a folder
uploaded = files.upload()

!unzip /content/archive.zip -d .

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Emotion Recognition using LSTM & One Hot</h1></span>

Emotion recognition is a task in affective computing that involves identifying and categorizing human emotions based on various inputs, such as speech, text, image, or video. Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) that is commonly used for emotion recognition tasks due to its ability to capture long-term dependencies in sequential data.

In this notebook, we will use **One-Hot Vector Representations** as the **Embeddings** of the Tokens
"""

import os, sys, random, time, datetime, re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tqdm

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import f1_score


import nltk #natural language toolkit
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Configuration File </h1></span>

It is very similar to yaml file. We are using a config class instead of yaml file. In the config file, we are setting all the hyperparameters and essential variable assignment
"""

# Hyper parameters etc. #(in class type)
class CFG:
    debug = True
    n_epochs = 25
    learning_rate = 1.0e-3
    batch_size = 64
    target_cols=['label'] # target columns
    seed = 42

    # Model parameters
    vocab_size = None
    embedding_dim = 256
    hidden_dim = 128
    dropout_rate = 0.25
    lstm_dropout_rate = 0.2,
    num_lstm_layers = 1
    num_class = None

# Set random seeds for reproducibility
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

seed_everything(CFG.seed)

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Dataset Loading </h1></span>

Let's load the dataset and see the category distribution of the dataset.
"""

# ====================================================
# Data Loading
# ====================================================

train = pd.read_csv('/content/train.txt', names=['text', 'emotion'], sep=';')
valid = pd.read_csv('/content/val.txt', names=['text', 'emotion'], sep=';')
test = pd.read_csv('/content/test.txt', names=['text', 'emotion'], sep=';')


print(f"train.shape: {train.shape}")
display(train.head())

print(f"valid.shape: {valid.shape}")
display(valid.head())

print(f"test.shape: {test.shape}")
display(test.head())

print(train["emotion"].unique())
CFG.num_class = train["emotion"].nunique()
print(CFG.num_class)

# ====================================================
# Label Mapping
# ====================================================

mapping = {'sadness' : 0,
           'anger' : 1,
           'love' : 2,
           'surprise' : 3,
           'fear' : 4,
           'joy' : 5,
          }

train['label'] = train['emotion'].map(mapping)
valid['label'] = valid['emotion'].map(mapping)
test['label'] = test['emotion'].map(mapping)

display(train.head())

import seaborn as sns
import matplotlib.pyplot as plt

def countplot(fig_x,fig_y,col,top_x,rotation,xlabel,title) :
    plt.figure(figsize=(fig_x,fig_y))
    total = float(len(train))
    ax = sns.countplot(x=col,  data=train, order = train[col].value_counts().iloc[:top_x].index)
    for p in ax.patches:
        percentage = '{:.1f}%'.format(100 * p.get_height()/total)
        x = p.get_x() + p.get_width() / 2 - 0.05
        y = p.get_y() + p.get_height()
        ax.annotate(percentage, (x, y),xytext = (x+0.05,y+20),ha = 'center',weight = 'bold',size = 11)
    plt.xticks(rotation = rotation,weight='bold',size = 10)
    plt.yticks(weight='bold',size = 10)
    plt.xlabel(xlabel,weight='bold',size = 12)
    plt.ylabel('count',weight='bold',size = 12)
    plt.title(title,weight='bold',size = 15)
    plt.show()

countplot(10,5,'emotion',15,45,'Emotion Class','Class Distribution')

# Considering a small set of data as input while debugging
if CFG.debug:
    CFG.epochs = 2
    train = train.sample(frac =.25)

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Text Preprocessing </h1></span>

We are doing basic text preprocessing like removing space, unncessary symbols, numbers and so on. To tokenize our sentence, we use a **space tokenizer** to get the tokens of the sentence.
"""

def preprocess_text(text):

    REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')    # add/remove regex as required
    SYMBOLS_RE = re.compile('[^0-9a-z #+_]') # unnecessary symbols
    NUMBERS = re.compile('\d+') # numbers
    STOPWORDS = set(stopwords.words('english')) # stopwords
    lemmatizer = WordNetLemmatizer()  # lemmatizer

    # clean
    text = text.lower() # lowering the text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # removing punc
    text = SYMBOLS_RE.sub('', text) # removing unnecessary symbols
    text = NUMBERS.sub('', text) # removing stop words

    # remove stopwords and lemmatize
    tokens = [word for word in text.split() if word not in STOPWORDS]
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return tokens

train['tokenized'] = train['text'].apply(preprocess_text)
valid['tokenized'] = valid['text'].apply(preprocess_text)
test['tokenized']  =  test['text'].apply(preprocess_text)

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Making the vocabulary </h1></span>

Now to get the token_ids for the tokens, we need to define a vocabulary. A vocabulary is created to map words to numerical indices. This is essential for several reasons:

**Numerical Representation**: Neural networks, including LSTMs, operate on numerical data. Therefore, text data, which is inherently symbolic (consisting of words), needs to be converted into a numerical format. Each unique word is assigned a unique index in the vocabulary.

**Embedding Layer Input**: LSTMs typically operate on word embeddings. An embedding layer maps each word index to a high-dimensional vector (embedding). This layer essentially learns a representation for each word in the vocabulary, and these representations are updated during training. The embedding layer requires a vocabulary to map word indices to corresponding embedding vectors.

**Fixed Input Size**: LSTMs process sequences of fixed length. By using a fixed vocabulary, you establish a consistent way to map words to indices, ensuring that each input sequence has the same length. Padding or truncation can be used to achieve this fixed length.
"""

def make_vocabulary_from_tokens(tokenized_sentences, min_doc_freq = 1,max_doc_freq = 1_000_000):

    # Count frequency of each token in dataset
    document_freq = {}
    for tokenized_sentence in tokenized_sentences:
        for token in tokenized_sentence:
            document_freq[token] = document_freq.get(token, 0) + 1

    # Discard tokens with freq < min_doc_freq
    qualified_tokens = {
        token: freq for token, freq in document_freq.items() if (min_doc_freq < freq < max_doc_freq)
    }

    # Add in token_ids for each token
    vocab = {token: token_id+2 for token_id, token in enumerate(qualified_tokens.keys())}

    # Add special tokens
    vocab['[PAD]'] = 0
    vocab['[UNK]'] = 1

    return vocab, qualified_tokens

vocab, doc_freq = make_vocabulary_from_tokens(train['tokenized'], 3)    # use only train set for this
CFG.vocab_size = len(vocab)
print(f'Total length of the Vocabulary: {len(vocab)}')

"""### Make a smaller vocabulary
Only words with frequencies 10 < doc_freq < 300 are kept. Reason ?
"""

small_vocab, _ = make_vocabulary_from_tokens(train['tokenized'], 10, 3000)
CFG.vocab_size = len(small_vocab)
print(f'Total length of the Vocabulary: {len(small_vocab)}')

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Tokens to One-Hot Encoding </h1></span>

Input IDs are numerical representations of tokens. Each unique token in a dataset is assigned a unique numerical identifier, called its input ID. These input IDs are used as indices to look up corresponding embeddings in the model's embedding layer.
"""

def one_hot_encode(tokenized_sentence):
    integer_coded = [small_vocab.get(token, 1) for token in tokenized_sentence]
    one_hot = []
    for token_id in integer_coded:
        embedding = np.zeros(CFG.vocab_size)
        embedding[token_id] = 1
        one_hot.append(embedding.tolist())
    return one_hot

X_train, y_train = train['tokenized'].apply(one_hot_encode), train['label'].to_list()
X_valid, y_valid = valid['tokenized'].apply(one_hot_encode), valid['label'].to_list()
X_test,  y_test  = test['tokenized'].apply(one_hot_encode),  test['label'].to_list()

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Creating Pytorch Dataset </h1></span>

We are creating a pytorch dataset instance. Basically, in deep learning, we need all data-samples in a batch should be should be the **same legnth**. In NLP, the pad_sequence function, commonly used in frameworks like PyTorch, is essential for handling sequences of variable lengths. It addresses the need for fixed-length input sequences, a requirement for batch processing in neural networks. Sequences of different lengths, prevalent in natural language data, are padded with a special token to ensure uniformity within batches. This step is crucial for maintaining consistent tensor shapes, aligning with the expectations of neural network models, and facilitating efficient batch processing. The pad_sequence function simplifies this padding process, ensuring that shorter sequences are padded appropriately for compatibility with deep learning models.
"""

class TextDataset(Dataset):
    def __init__(self, input_ids, labels):
        '''
        - Stores tokenized sentences as tensors of input ids according to vocabulary mapping.
        - Label indices are converted to one hot encoding.
        '''
        self.input_ids = pad_sequence([torch.tensor(sequence) for sequence in input_ids], batch_first=True)
        self.labels = nn.functional.one_hot(torch.tensor(labels)).to(torch.float)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return [self.input_ids[idx], self.labels[idx]]

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Creating the Model </h1></span>

We are following the same model architecure shown in slide. Basically,

- Initialize the LSTMClassifier, setting the number of LSTM layers, hidden dimensions, vocabulary size, embedding dimensions, and dropout rate.


- Configure the LSTM layer using nn.LSTM, with parameters including input and hidden dimensions, the number of layers, and batch-first set to True.

- Create a fully connected layer (nn.Linear) for the classification task, with the output size equal to the number of classes.

- In the forward method, apply dropout to the input embeddings, then pass them through the LSTM layer.

- Extract the representation of the last token as the sentence representation from the LSTM output.

- Pass the sentence representation through the fully connected layer to obtain the final logits for classification.

- Return the logits as the output of the forward pass.
"""

class LSTMClassifierOHE(nn.Module):

    def __init__(self, cfg):
        # Model
        super().__init__()
        self.n_layers = cfg.num_lstm_layers
        self.hidden_dim = cfg.hidden_dim
        self.lstm = nn.LSTM(
            cfg.vocab_size, cfg.hidden_dim, num_layers=self.n_layers, # vocab_size, hidden dim
            batch_first=True
        )
        self.dropout = nn.Dropout(cfg.dropout_rate)
        self.fc = nn.Linear(self.hidden_dim, cfg.num_class)

    def forward(self, X_batch):
        h, c = torch.randn(self.n_layers, len(X_batch), self.hidden_dim), torch.randn(self.n_layers, len(X_batch), self.hidden_dim)
        output, (h, c) = self.lstm(X_batch, (h, c))
        sentence_repr = output[:,-1] # considering the last token repr as the sentence repr
        logits = self.fc(sentence_repr)
        return logits

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Pytorch Code for One Epoch Training </h1></span>"""

from sklearn.metrics import f1_score

def get_score(y_trues, y_preds):
    y_predicted = y_preds.argmax(axis=1)  # Convert probabilities to class predictions
    macro_f1 = f1_score(y_trues, y_predicted, average='macro')
    return macro_f1


def train_one_epoch(train_loader, model, optimizer, loss_fn, epoch):
    model.train()
    running_loss = 0
    running_score = 0

    loop = tqdm.tqdm(enumerate(train_loader),
                         total=len(train_loader), leave=False)

    for i, data in loop:
        inputs, labels = data

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()

        # compute metrics and store
        score = get_score(labels.argmax(axis=1), outputs)
        running_score += score
        running_loss += loss.item()

        loop.set_description(f"Epoch [{epoch + 1}/{CFG.n_epochs}]")
        loop.set_postfix(loss=running_loss / (i + 1))

    avg_score = float(score / (i + 1))
    avg_loss = float(loss / (i + 1))

    return avg_score, avg_loss


def eval_one_epoch(valid_loader, model, loss_fn):
    # Validate
    model.eval()
    running_vloss = 0
    running_vscore = 0

    with torch.no_grad():
        for i, vdata in enumerate(valid_loader):
            vinputs, vlabels = vdata
            voutputs = model(vinputs)
            vloss = loss_fn(voutputs, vlabels)

            vscore = get_score(vlabels.argmax(axis=1), voutputs)
            running_vscore += vscore
            running_vloss += vloss

    avg_vscore = float(running_vscore / (i + 1))
    avg_vloss = float(running_vloss / (i + 1))

    return avg_vscore, avg_vloss

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Training Loop </h1></span>"""

def train_loop(X_train, y_train, X_valid, y_valid, CFG):

    model = LSTMClassifierOHE(CFG) # Initializing the Model

    # Loss function and optimizer
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)

    train_dataset = TextDataset(X_train, y_train)
    valid_dataset = TextDataset(X_valid, y_valid)

    train_loader  = DataLoader(train_dataset,batch_size=CFG.batch_size, shuffle=True)
    valid_loader  = DataLoader(valid_dataset,batch_size=CFG.batch_size, shuffle=False)

    best_score = 0
    train_loss = []; train_score = []
    valid_loss = []; valid_score = []

    for epoch in range(CFG.n_epochs):

        avg_score, avg_loss = train_one_epoch(train_loader, model, optimizer, loss_fn, epoch)

        avg_vscore, avg_vloss = eval_one_epoch(valid_loader, model, loss_fn)

        # Track best performance and save model's state
        if avg_vscore > best_score:
            best_score = avg_vscore
            model_path = f"best_scored_model.pth"
            torch.save(model.state_dict(), model_path)

        print(f"Epoch {epoch+1}: Training Loss = {avg_loss:.4} Training Score = {avg_score:.4f}", end = " & ")
        print(f"Validation Loss = {avg_vloss:.4f} Validation Score = {avg_vscore:.4f}")
        train_loss.append(avg_loss); train_score.append(avg_score)
        valid_loss.append(avg_vloss); valid_score.append(avg_vscore)

    # Plot loss and metric
    fig,(ax1, ax2) = plt.subplots(1, 2)

    ax1.set_xlabel('epoch'); ax1.set_ylabel('loss'); ax1.set_title('training loss')
    ax1.plot(np.arange(len(train_loss)), train_loss, label='training')
    ax1.plot(np.arange(len(valid_loss)), valid_loss, label='validation')
    ax1.legend()

    ax2.set_xlabel('epoch'); ax2.set_ylabel('score'); ax2.set_title('training score')
    ax2.plot(np.arange(len(train_score)), train_score, label='training')
    ax2.plot(np.arange(len(valid_score)), valid_score, label='validation')
    ax2.legend()

    return best_score

# ====================================================
# the training
# ====================================================

if __name__ == '__main__':
    best_score = train_loop(X_train, y_train, X_valid, y_valid, CFG)

"""# <span><h1 style = "font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#C01F4D; border-radius: 100px 100px; text-align:center"> Prediction on Test Dataset & Evaluation </h1></span>"""

def test_model(X, y, model, label_names):
    model.eval()

    test_dataset = TextDataset(X, y)
    test_loader  = DataLoader(test_dataset,batch_size=CFG.batch_size, shuffle=False)

    y_preds = []
    with torch.no_grad():
        for i, data in tqdm.tqdm(enumerate(test_loader)):
            inputs, labels = data
            outputs = model(inputs)
            y_preds.append(outputs.argmax(1))

    y_preds = torch.cat(y_preds)

    print(classification_report(y, y_preds, target_names=label_names))
    ConfusionMatrixDisplay.from_predictions(y, y_preds, display_labels=label_names)

    return y_preds

label_names = train["emotion"].unique()
model = LSTMClassifierOHE(CFG)
model_path = f"best_scored_model.pth"
state = torch.load(model_path) # loading the saved model

model.load_state_dict(state)
test_set_predictions = test_model(X_test, y_test, model, label_names)