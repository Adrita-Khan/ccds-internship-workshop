# -*- coding: utf-8 -*-
"""CCDS Assignment 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oY7yZIsmLAZoAQ7uUdziTYf4JGtH2aUJ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, f1_score
import seaborn as sns
from tqdm import tqdm
import random
import torch.nn.functional as F
import cv2

# ===========================
# Configuration and Settings
# ===========================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

config = {
    'batch_size': 1024,
    'learning_rate': 0.001,
    'num_epochs': 20,
    'validation_split': 0.2,
    'seed': 42,
    'num_workers': 4,
    'patience': 5,
}

# ===========================
# Data Preparation
# ===========================
transform_train = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_val_dataset = datasets.FashionMNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform_train
)

test_dataset = datasets.FashionMNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform_test
)

train_size = int((1 - config['validation_split']) * len(train_val_dataset))
val_size = len(train_val_dataset) - train_size
train_set, val_set = random_split(
    train_val_dataset,
    [train_size, val_size],
    generator=torch.Generator().manual_seed(config['seed'])
)

# For validation set, use test transform
val_set.dataset.transform = transform_test

train_loader = DataLoader(
    train_set,
    batch_size=config['batch_size'],
    shuffle=True,
    num_workers=config['num_workers'],
    pin_memory=True
)

val_loader = DataLoader(
    val_set,
    batch_size=config['batch_size'],
    shuffle=False,
    num_workers=config['num_workers'],
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=config['batch_size'],
    shuffle=False,
    num_workers=config['num_workers'],
    pin_memory=True
)

class_labels = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# ===========================
# Model Definition for CAM
# ===========================
class FashionCAM(nn.Module):
    def __init__(self, num_classes=10):
        super(FashionCAM, self).__init__()
        # According to given architecture: final shape after last conv block is (7x7x3)
        self.features = nn.Sequential(
            nn.Conv2d(1, 3, kernel_size=3, padding=1),
            nn.BatchNorm2d(3),
            nn.ReLU(inplace=True),

            nn.Conv2d(3, 5, kernel_size=3, padding=1),
            nn.BatchNorm2d(5),
            nn.ReLU(inplace=True),

            nn.MaxPool2d(2, 2),  # 14x14x5

            nn.Conv2d(5, 3, kernel_size=3, padding=1),
            nn.BatchNorm2d(3),
            nn.ReLU(inplace=True),

            nn.MaxPool2d(2, 2)  # 7x7x3
        )

        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d((1,1))
        # Single FC for class logits
        self.fc = nn.Linear(3, num_classes)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.features(x)
        self.last_conv_output = x  # store for CAM generation
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

model = FashionCAM(num_classes=10).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])

# ===========================
# Early Stopping
# ===========================
class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0.0, path='checkpoint.pt'):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.path = path
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        if self.verbose:
            print(f'Validation loss improved, saving model ...')
        torch.save(model.state_dict(), self.path)

early_stopping = EarlyStopping(patience=config['patience'], verbose=True)

# ===========================
# Training and Validation
# ===========================
def train_validate(model, criterion, optimizer, num_epochs, train_loader, val_loader, device, early_stopping):
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)

        # Training phase
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for images, labels in tqdm(train_loader, desc='Training', leave=False):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs, 1)
            correct_train += (predicted == labels).sum().item()
            total_train += labels.size(0)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_accuracy = 100 * correct_train / total_train
        train_losses.append(epoch_train_loss)

        # Validation phase
        model.eval()
        val_running_loss = 0.0
        correct_val = 0
        total_val = 0

        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc='Validation', leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_running_loss += loss.item() * images.size(0)
                _, predicted = torch.max(outputs, 1)
                correct_val += (predicted == labels).sum().item()
                total_val += labels.size(0)

        epoch_val_loss = val_running_loss / len(val_loader.dataset)
        val_accuracy = 100 * correct_val / total_val
        val_losses.append(epoch_val_loss)

        print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')
        print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')

        # Early Stopping
        early_stopping(epoch_val_loss, model)
        if early_stopping.early_stop:
            print("Early stopping triggered.")
            break

    # Load best model
    model.load_state_dict(torch.load(early_stopping.path))

    return {
        'train_losses': train_losses,
        'val_losses': val_losses
    }

history = train_validate(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    num_epochs=config['num_epochs'],
    train_loader=train_loader,
    val_loader=val_loader,
    device=device,
    early_stopping=early_stopping
)

# ===========================
# Plot Training/Validation Loss
# ===========================
def plot_training_history(history):
    epochs = range(1, len(history['train_losses']) + 1)
    plt.figure(figsize=(10,5))
    plt.plot(epochs, history['train_losses'], label='Train Loss')
    plt.plot(epochs, history['val_losses'], label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training & Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_training_history(history)

# ===========================
# Evaluation on Test Data
# ===========================
def evaluate_model(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for images, labels in tqdm(loader, desc='Testing', leave=False):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    accuracy = 100 * correct / total
    f1 = f1_score(all_labels, all_preds, average='weighted')
    print(f"Test Accuracy: {accuracy:.2f}%")
    print(f"Test F1 Score: {f1:.4f}")

    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10,8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels,
                yticklabels=class_labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return accuracy, f1, cm

test_accuracy, test_f1, cm = evaluate_model(model, test_loader, device)

# Save the final model
torch.save(model.state_dict(), 'fashion_cnn_modified_for_CAM.pth')
print("Model saved as fashion_cnn_modified_for_CAM.pth")

# ===========================
# CAM Generation Functions
# ===========================

def generate_cam(feature_maps, fc_weights, class_idx):
    """
    Generate a Class Activation Map (CAM).
    feature_maps: (C, H, W) on device (likely GPU)
    fc_weights: (num_classes, C) likely on CPU, need to move it to device
    class_idx: class index
    """
    # Move fc_weights to the same device as feature_maps
    fc_weights = fc_weights.to(feature_maps.device)
    class_weights = fc_weights[class_idx]  # (C)

    # Create cam on the same device
    cam = torch.zeros(feature_maps.shape[1], feature_maps.shape[2], device=feature_maps.device)
    for c in range(feature_maps.shape[0]):
        cam += class_weights[c] * feature_maps[c, :, :]

    # Move cam back to CPU for normalization and visualization
    cam = cam.detach().cpu().numpy()
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
    return cam


def visualize_cam_on_image(img, cam, alpha=0.5):
    """
    Overlays the CAM on the image.
    img: (H,W) numpy grayscale image
    cam: (H,W) CAM heatmap (before resizing)
    """
    # Resize CAM to match img size (28x28)
    H, W = img.shape
    cam_resized = cv2.resize(cam, (W, H), interpolation=cv2.INTER_LINEAR)

    # Convert grayscale to RGB
    img = (img * 255).astype(np.uint8)
    img = np.stack([img, img, img], axis=2)

    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)
    heatmap = np.float32(heatmap) / 255
    overlayed = heatmap * alpha + np.float32(img) / 255 * (1 - alpha)
    overlayed = np.clip(overlayed, 0, 1)
    return (overlayed * 255).astype(np.uint8)

# Extract FC weights
fc_weights = model.fc.weight.data  # shape: (num_classes, 3)

def get_single_image_cam(model, image, label, class_labels):
    model.eval()
    with torch.no_grad():
        image = image.unsqueeze(0).to(device)  # (1,1,28,28)
        outputs = model(image)
        probs = F.softmax(outputs, dim=1)
        pred_class = torch.argmax(probs, dim=1).item()
        pred_prob = probs[0, pred_class].item()
        gt_class = label.item()

    # Feature maps (C,H,W)
    feature_maps = model.last_conv_output[0]

    # CAM for predicted class
    cam_pred = generate_cam(feature_maps, fc_weights, pred_class)

    # CAM for ground truth class if needed (when pred != gt)
    cam_gt = generate_cam(feature_maps, fc_weights, gt_class) if gt_class != pred_class else None

    return pred_class, pred_prob, cam_pred, cam_gt

def show_result(image, label, pred_class, pred_prob, cam_pred, cam_gt, class_labels):
    # Convert image tensor to numpy
    img_np = image.numpy().squeeze()  # shape (28,28)
    plt.figure(figsize=(10, 10))

    if cam_gt is None:
        # For correct predictions: show input image + predicted CAM
        # 1st: input image
        plt.subplot(1, 2, 1)
        plt.imshow(img_np, cmap='gray')
        plt.title(f"Input Image\nGT: {class_labels[label.item()]}\nPred: {class_labels[pred_class]} ({pred_prob:.2f})")
        plt.axis('off')

        # 2nd: CAM for predicted class
        overlay_pred = visualize_cam_on_image(img_np, cam_pred)
        plt.subplot(1, 2, 2)
        plt.imshow(overlay_pred)
        plt.title(f"CAM for Predicted: {class_labels[pred_class]}")
        plt.axis('off')

    else:
        # For incorrect predictions: show input image, gt CAM, pred CAM
        # 1st row: input image
        plt.subplot(1, 3, 1)
        plt.imshow(img_np, cmap='gray')
        plt.title(f"Input Image\nGT: {class_labels[label.item()]}\nPred: {class_labels[pred_class]} ({pred_prob:.2f})")
        plt.axis('off')

        # 2nd: CAM for ground truth class
        overlay_gt = visualize_cam_on_image(img_np, cam_gt)
        plt.subplot(1, 3, 2)
        plt.imshow(overlay_gt)
        plt.title(f"CAM for Ground Truth: {class_labels[label.item()]}")
        plt.axis('off')

        # 3rd: CAM for predicted class
        overlay_pred = visualize_cam_on_image(img_np, cam_pred)
        plt.subplot(1, 3, 3)
        plt.imshow(overlay_pred)
        plt.title(f"CAM for Predicted: {class_labels[pred_class]}")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# ===========================
# Select Samples for CAM Visualization
# ===========================
model.eval()
all_images = []
all_labels = []
all_preds = []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        preds = torch.argmax(outputs, dim=1)
        all_images.append(images.cpu())
        all_labels.append(labels.cpu())
        all_preds.append(preds.cpu())

all_images = torch.cat(all_images)
all_labels = torch.cat(all_labels)
all_preds = torch.cat(all_preds)

correct_idx = (all_preds == all_labels)
incorrect_idx = (all_preds != all_labels)

correct_images = all_images[correct_idx]
correct_labels = all_labels[correct_idx]
correct_preds = all_preds[correct_idx]

incorrect_images = all_images[incorrect_idx]
incorrect_labels = all_labels[incorrect_idx]
incorrect_preds = all_preds[incorrect_idx]

# For the 10 correct predictions (one from each class)
picked_correct = []
class_found = set()
for i in range(len(correct_images)):
    c = correct_labels[i].item()
    if c not in class_found:
        picked_correct.append(i)
        class_found.add(c)
    if len(class_found) == 10:
        break

# For the 30 incorrect predictions
if len(incorrect_images) > 30:
    picked_incorrect = np.random.choice(range(len(incorrect_images)), size=30, replace=False)
else:
    # If there are less than 30 incorrect predictions (unlikely), just use all
    picked_incorrect = range(len(incorrect_images))

print("Showing CAMs for 10 correct predictions (one from each class):")
for idx in picked_correct:
    image = correct_images[idx]
    label = correct_labels[idx]
    pred_class, pred_prob, cam_pred, _ = get_single_image_cam(model, image, label, class_labels)
    show_result(image, label, pred_class, pred_prob, cam_pred, None, class_labels)

print("Showing CAMs for 30 incorrect predictions:")
count = 0
for idx in picked_incorrect:
    image = incorrect_images[idx]
    label = incorrect_labels[idx]
    pred_class, pred_prob, cam_pred, cam_gt = get_single_image_cam(model, image, label, class_labels)
    show_result(image, label, pred_class, pred_prob, cam_pred, cam_gt, class_labels)
    count += 1
    if count >= 30:
        break

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, f1_score
import seaborn as sns
from tqdm import tqdm
import random
import torch.nn.functional as F
import cv2

# ===========================
# Configuration and Settings
# ===========================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

config = {
    'batch_size': 512,  # smaller batch size might help a bit with convergence
    'learning_rate': 0.001,
    'num_epochs': 30,  # increase epochs
    'validation_split': 0.2,
    'seed': 42,
    'num_workers': 4,
    'patience': 7,  # slightly more patience
}

# ===========================
# Data Preparation
# ===========================
# According to the FashionMNIST documentation, mean ~0.2860 and std ~0.3530
mean_val = 0.2860
std_val = 0.3530

# Optional: add a slight data augmentation for training
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),  # data augmentation
    transforms.ToTensor(),
    transforms.Normalize((mean_val,), (std_val,))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((mean_val,), (std_val,))
])

train_val_dataset = datasets.FashionMNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform_train
)

test_dataset = datasets.FashionMNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform_test
)

train_size = int((1 - config['validation_split']) * len(train_val_dataset))
val_size = len(train_val_dataset) - train_size
train_set, val_set = random_split(
    train_val_dataset,
    [train_size, val_size],
    generator=torch.Generator().manual_seed(config['seed'])
)

val_set.dataset.transform = transform_test

train_loader = DataLoader(
    train_set,
    batch_size=config['batch_size'],
    shuffle=True,
    num_workers=config['num_workers'],
    pin_memory=True
)

val_loader = DataLoader(
    val_set,
    batch_size=config['batch_size'],
    shuffle=False,
    num_workers=config['num_workers'],
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=config['batch_size'],
    shuffle=False,
    num_workers=config['num_workers'],
    pin_memory=True
)

class_labels = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# ===========================
# Model Definition for CAM
# ===========================
class FashionCAM(nn.Module):
    def __init__(self, num_classes=10):
        super(FashionCAM, self).__init__()
        # Increase number of filters to improve capacity
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 14x14x64

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # 7x7x128
        )

        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d((1,1))
        # Single FC for class logits
        self.fc = nn.Linear(128, num_classes)
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.features(x)
        self.last_conv_output = x  # store for CAM generation (C,H,W)
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

model = FashionCAM(num_classes=10).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])

# Introduce a learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

# ===========================
# Early Stopping
# ===========================
class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0.0, path='checkpoint.pt'):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.path = path
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        if self.verbose:
            print(f'Validation loss improved, saving model ...')
        torch.save(model.state_dict(), self.path)

early_stopping = EarlyStopping(patience=config['patience'], verbose=True)

# ===========================
# Training and Validation
# ===========================
def train_validate(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, device, early_stopping):
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)

        # Training phase
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for images, labels in tqdm(train_loader, desc='Training', leave=False):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs, 1)
            correct_train += (predicted == labels).sum().item()
            total_train += labels.size(0)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_accuracy = 100 * correct_train / total_train
        train_losses.append(epoch_train_loss)

        # Validation phase
        model.eval()
        val_running_loss = 0.0
        correct_val = 0
        total_val = 0

        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc='Validation', leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_running_loss += loss.item() * images.size(0)
                _, predicted = torch.max(outputs, 1)
                correct_val += (predicted == labels).sum().item()
                total_val += labels.size(0)

        epoch_val_loss = val_running_loss / len(val_loader.dataset)
        val_accuracy = 100 * correct_val / total_val
        val_losses.append(epoch_val_loss)

        # Step scheduler
        scheduler.step(epoch_val_loss)

        print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')
        print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')

        # Early Stopping
        early_stopping(epoch_val_loss, model)
        if early_stopping.early_stop:
            print("Early stopping triggered.")
            break

    # Load best model
    model.load_state_dict(torch.load(early_stopping.path))

    return {
        'train_losses': train_losses,
        'val_losses': val_losses
    }

history = train_validate(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    num_epochs=config['num_epochs'],
    train_loader=train_loader,
    val_loader=val_loader,
    device=device,
    early_stopping=early_stopping
)

# ===========================
# Plot Training/Validation Loss
# ===========================
def plot_training_history(history):
    epochs = range(1, len(history['train_losses']) + 1)
    plt.figure(figsize=(10,5))
    plt.plot(epochs, history['train_losses'], label='Train Loss')
    plt.plot(epochs, history['val_losses'], label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training & Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_training_history(history)

# ===========================
# Evaluation on Test Data
# ===========================
def evaluate_model(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for images, labels in tqdm(loader, desc='Testing', leave=False):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    accuracy = 100 * correct / total
    f1 = f1_score(all_labels, all_preds, average='weighted')
    print(f"Test Accuracy: {accuracy:.2f}%")
    print(f"Test F1 Score: {f1:.4f}")

    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10,8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels,
                yticklabels=class_labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return accuracy, f1, cm

test_accuracy, test_f1, cm = evaluate_model(model, test_loader, device)

# Save the final model
torch.save(model.state_dict(), 'fashion_cnn_modified_for_CAM.pth')
print("Model saved as fashion_cnn_modified_for_CAM.pth")

# ===========================
# CAM Generation Functions
# (Same as before, just adjusting fc_weights retrieval)
# ===========================
def generate_cam(feature_maps, fc_weights, class_idx):
    fc_weights = fc_weights.to(feature_maps.device)
    class_weights = fc_weights[class_idx]
    cam = torch.zeros(feature_maps.shape[1], feature_maps.shape[2], device=feature_maps.device)
    for c in range(feature_maps.shape[0]):
        cam += class_weights[c] * feature_maps[c, :, :]
    cam = cam.detach().cpu().numpy()
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
    return cam

def visualize_cam_on_image(img, cam, alpha=0.5):
    H, W = img.shape
    cam_resized = cv2.resize(cam, (W, H), interpolation=cv2.INTER_LINEAR)
    img = (img * std_val + mean_val) * 255.0  # denormalize for visualization
    img = np.clip(img, 0, 255).astype(np.uint8)
    img = np.stack([img, img, img], axis=2)
    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)
    heatmap = np.float32(heatmap) / 255
    overlayed = heatmap * alpha + np.float32(img) / 255 * (1 - alpha)
    overlayed = np.clip(overlayed, 0, 1)
    return (overlayed * 255).astype(np.uint8)

fc_weights = model.fc.weight.data

def get_single_image_cam(model, image, label, class_labels):
    model.eval()
    with torch.no_grad():
        image = image.unsqueeze(0).to(device)
        outputs = model(image)
        probs = F.softmax(outputs, dim=1)
        pred_class = torch.argmax(probs, dim=1).item()
        pred_prob = probs[0, pred_class].item()
        gt_class = label.item()

    feature_maps = model.last_conv_output[0]

    cam_pred = generate_cam(feature_maps, fc_weights, pred_class)
    cam_gt = generate_cam(feature_maps, fc_weights, gt_class) if gt_class != pred_class else None

    return pred_class, pred_prob, cam_pred, cam_gt

def show_result(image, label, pred_class, pred_prob, cam_pred, cam_gt, class_labels):
    img_np = image.numpy().squeeze()
    plt.figure(figsize=(10, 10))

    if cam_gt is None:
        # correct prediction
        plt.subplot(1, 2, 1)
        plt.imshow((img_np*std_val + mean_val), cmap='gray')  # denormalized
        plt.title(f"Input\nGT: {class_labels[label.item()]}\nPred: {class_labels[pred_class]} ({pred_prob:.2f})")
        plt.axis('off')

        overlay_pred = visualize_cam_on_image(img_np, cam_pred)
        plt.subplot(1, 2, 2)
        plt.imshow(overlay_pred)
        plt.title(f"CAM: {class_labels[pred_class]}")
        plt.axis('off')
    else:
        # incorrect prediction
        plt.subplot(1, 3, 1)
        plt.imshow((img_np*std_val + mean_val), cmap='gray')
        plt.title(f"Input\nGT: {class_labels[label.item()]}\nPred: {class_labels[pred_class]} ({pred_prob:.2f})")
        plt.axis('off')

        overlay_gt = visualize_cam_on_image(img_np, cam_gt)
        plt.subplot(1, 3, 2)
        plt.imshow(overlay_gt)
        plt.title(f"CAM GT: {class_labels[label.item()]}")
        plt.axis('off')

        overlay_pred = visualize_cam_on_image(img_np, cam_pred)
        plt.subplot(1, 3, 3)
        plt.imshow(overlay_pred)
        plt.title(f"CAM Pred: {class_labels[pred_class]}")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# ===========================
# Select Samples for CAM Visualization
# ===========================
model.eval()
all_images = []
all_labels = []
all_preds = []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        preds = torch.argmax(outputs, dim=1)
        all_images.append(images.cpu())
        all_labels.append(labels.cpu())
        all_preds.append(preds.cpu())

all_images = torch.cat(all_images)
all_labels = torch.cat(all_labels)
all_preds = torch.cat(all_preds)

correct_idx = (all_preds == all_labels)
incorrect_idx = (all_preds != all_labels)

correct_images = all_images[correct_idx]
correct_labels = all_labels[correct_idx]
correct_preds = all_preds[correct_idx]

incorrect_images = all_images[incorrect_idx]
incorrect_labels = all_labels[incorrect_idx]
incorrect_preds = all_preds[incorrect_idx]

# For the 10 correct predictions (one from each class)
picked_correct = []
class_found = set()
for i in range(len(correct_images)):
    c = correct_labels[i].item()
    if c not in class_found:
        picked_correct.append(i)
        class_found.add(c)
    if len(class_found) == 10:
        break

if len(incorrect_images) > 30:
    picked_incorrect = np.random.choice(range(len(incorrect_images)), size=30, replace=False)
else:
    picked_incorrect = range(len(incorrect_images))

print("Showing CAMs for 10 correct predictions (one from each class):")
for idx in picked_correct:
    image = correct_images[idx]
    label = correct_labels[idx]
    pred_class, pred_prob, cam_pred, _ = get_single_image_cam(model, image, label, class_labels)
    show_result(image, label, pred_class, pred_prob, cam_pred, None, class_labels)

print("Showing CAMs for 30 incorrect predictions:")
count = 0
for idx in picked_incorrect:
    image = incorrect_images[idx]
    label = incorrect_labels[idx]
    pred_class, pred_prob, cam_pred, cam_gt = get_single_image_cam(model, image, label, class_labels)
    show_result(image, label, pred_class, pred_prob, cam_pred, cam_gt, class_labels)
    count += 1
    if count >= 30:
        break

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, f1_score
import seaborn as sns
from tqdm import tqdm
import random
import torch.nn.functional as F
import cv2
import os

# ===========================
# Configuration and Settings
# ===========================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

config = {
    'batch_size': 512,
    'learning_rate': 0.001,
    'num_epochs': 30,
    'validation_split': 0.2,
    'seed': 42,
    'num_workers': 4,
    'patience': 7
}

mean_val = 0.2860
std_val = 0.3530

# ===========================
# Data Preparation
# ===========================
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomCrop(28, padding=2),
    transforms.ToTensor(),
    transforms.Normalize((mean_val,), (std_val,))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((mean_val,), (std_val,))
])

train_val_dataset = datasets.FashionMNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform_train
)

test_dataset = datasets.FashionMNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform_test
)

train_size = int((1 - config['validation_split']) * len(train_val_dataset))
val_size = len(train_val_dataset) - train_size
train_set, val_set = random_split(
    train_val_dataset,
    [train_size, val_size],
    generator=torch.Generator().manual_seed(config['seed'])
)

val_set.dataset.transform = transform_test

train_loader = DataLoader(
    train_set,
    batch_size=config['batch_size'],
    shuffle=True,
    num_workers=config['num_workers'],
    pin_memory=True
)

val_loader = DataLoader(
    val_set,
    batch_size=config['batch_size'],
    shuffle=False,
    num_workers=config['num_workers'],
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=config['batch_size'],
    shuffle=False,
    num_workers=config['num_workers'],
    pin_memory=True
)

class_labels = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# ===========================
# Model Definition for CAM
# ===========================
class FashionCAM(nn.Module):
    def __init__(self, num_classes=10):
        super(FashionCAM, self).__init__()
        # Increased capacity and added dropout
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 14x14x64
            nn.Dropout(p=0.1),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # 7x7x128
        )

        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d((1,1))
        # Single FC for class logits
        self.fc = nn.Linear(128, num_classes)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.features(x)
        self.last_conv_output = x  # store for CAM generation
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

model = FashionCAM(num_classes=10).to(device)
criterion = nn.CrossEntropyLoss()

# Add weight decay to reduce overfitting
optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-4)

# Scheduler to reduce LR on plateau
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

# ===========================
# Early Stopping
# ===========================
class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0.0, path='checkpoint.pt'):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.path = path
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        if self.verbose:
            print(f'Validation loss improved, saving model ...')
        torch.save(model.state_dict(), self.path)

early_stopping = EarlyStopping(patience=config['patience'], verbose=True)

# ===========================
# Training and Validation
# ===========================
def train_validate(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, device, early_stopping):
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)

        # Training phase
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for images, labels in tqdm(train_loader, desc='Training', leave=False):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs, 1)
            correct_train += (predicted == labels).sum().item()
            total_train += labels.size(0)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_accuracy = 100 * correct_train / total_train
        train_losses.append(epoch_train_loss)

        # Validation phase
        model.eval()
        val_running_loss = 0.0
        correct_val = 0
        total_val = 0

        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc='Validation', leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_running_loss += loss.item() * images.size(0)
                _, predicted = torch.max(outputs, 1)
                correct_val += (predicted == labels).sum().item()
                total_val += labels.size(0)

        epoch_val_loss = val_running_loss / len(val_loader.dataset)
        val_accuracy = 100 * correct_val / total_val
        val_losses.append(epoch_val_loss)

        # Step scheduler
        scheduler.step(epoch_val_loss)

        print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')
        print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')

        # Early Stopping
        early_stopping(epoch_val_loss, model)
        if early_stopping.early_stop:
            print("Early stopping triggered.")
            break

    # Load best model
    model.load_state_dict(torch.load(early_stopping.path))

    return {
        'train_losses': train_losses,
        'val_losses': val_losses
    }

history = train_validate(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    num_epochs=config['num_epochs'],
    train_loader=train_loader,
    val_loader=val_loader,
    device=device,
    early_stopping=early_stopping
)

# ===========================
# Plot Training/Validation Loss
# ===========================
def plot_training_history(history):
    epochs = range(1, len(history['train_losses']) + 1)
    plt.figure(figsize=(10,5))
    plt.plot(epochs, history['train_losses'], label='Train Loss')
    plt.plot(epochs, history['val_losses'], label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training & Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_training_history(history)

# ===========================
# Evaluation on Test Data
# ===========================
def evaluate_model(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for images, labels in tqdm(loader, desc='Testing', leave=False):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    accuracy = 100 * correct / total
    f1 = f1_score(all_labels, all_preds, average='weighted')
    print(f"Test Accuracy: {accuracy:.2f}%")
    print(f"Test F1 Score: {f1:.4f}")

    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10,8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels,
                yticklabels=class_labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return accuracy, f1, cm

test_accuracy, test_f1, cm = evaluate_model(model, test_loader, device)

# Save the final model
model_path = 'fashion_cnn_modified_for_CAM.pth'
torch.save(model.state_dict(), model_path)
print(f"Model saved as {model_path}")

# ===========================
# CAM Generation Functions
# ===========================
def generate_cam(feature_maps, fc_weights, class_idx):
    fc_weights = fc_weights.to(feature_maps.device)
    class_weights = fc_weights[class_idx]
    cam = torch.zeros(feature_maps.shape[1], feature_maps.shape[2], device=feature_maps.device)
    for c in range(feature_maps.shape[0]):
        cam += class_weights[c] * feature_maps[c, :, :]
    cam = cam.detach().cpu().numpy()
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
    return cam

def visualize_cam_on_image(img, cam, alpha=0.5):
    H, W = img.shape
    cam_resized = cv2.resize(cam, (W, H), interpolation=cv2.INTER_LINEAR)

    # denormalize image for visualization
    img = img * std_val + mean_val
    img = np.clip(img * 255, 0, 255).astype(np.uint8)
    img = np.stack([img, img, img], axis=2)

    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)
    heatmap = np.float32(heatmap) / 255
    overlayed = heatmap * alpha + np.float32(img) / 255 * (1 - alpha)
    overlayed = np.clip(overlayed, 0, 1)
    return (overlayed * 255).astype(np.uint8)

fc_weights = model.fc.weight.data

def get_single_image_cam(model, image, label, class_labels):
    model.eval()
    with torch.no_grad():
        image = image.unsqueeze(0).to(device)
        outputs = model(image)
        probs = F.softmax(outputs, dim=1)
        pred_class = torch.argmax(probs, dim=1).item()
        pred_prob = probs[0, pred_class].item()
        gt_class = label.item()

    feature_maps = model.last_conv_output[0]
    cam_pred = generate_cam(feature_maps, fc_weights, pred_class)
    cam_gt = generate_cam(feature_maps, fc_weights, gt_class) if gt_class != pred_class else None

    return pred_class, pred_prob, cam_pred, cam_gt

def show_result(image, label, pred_class, pred_prob, cam_pred, cam_gt, class_labels):
    img_np = image.numpy().squeeze()
    plt.figure(figsize=(10, 10))

    if cam_gt is None:
        # correct prediction
        plt.subplot(1, 2, 1)
        plt.imshow(img_np*std_val+mean_val, cmap='gray')
        plt.title(f"Input\nGT: {class_labels[label.item()]}\nPred: {class_labels[pred_class]} ({pred_prob:.2f})")
        plt.axis('off')

        overlay_pred = visualize_cam_on_image(img_np, cam_pred)
        plt.subplot(1, 2, 2)
        plt.imshow(overlay_pred)
        plt.title(f"CAM: {class_labels[pred_class]}")
        plt.axis('off')
    else:
        # incorrect prediction
        plt.subplot(1, 3, 1)
        plt.imshow(img_np*std_val+mean_val, cmap='gray')
        plt.title(f"Input\nGT: {class_labels[label.item()]}\nPred: {class_labels[pred_class]} ({pred_prob:.2f})")
        plt.axis('off')

        overlay_gt = visualize_cam_on_image(img_np, cam_gt)
        plt.subplot(1, 3, 2)
        plt.imshow(overlay_gt)
        plt.title(f"CAM GT: {class_labels[label.item()]}")
        plt.axis('off')

        overlay_pred = visualize_cam_on_image(img_np, cam_pred)
        plt.subplot(1, 3, 3)
        plt.imshow(overlay_pred)
        plt.title(f"CAM Pred: {class_labels[pred_class]}")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# ===========================
# Select Samples for CAM Visualization
# ===========================
model.eval()
all_images = []
all_labels = []
all_preds = []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        preds = torch.argmax(outputs, dim=1)
        all_images.append(images.cpu())
        all_labels.append(labels.cpu())
        all_preds.append(preds.cpu())

all_images = torch.cat(all_images)
all_labels = torch.cat(all_labels)
all_preds = torch.cat(all_preds)

correct_idx = (all_preds == all_labels)
incorrect_idx = (all_preds != all_labels)

correct_images = all_images[correct_idx]
correct_labels = all_labels[correct_idx]

incorrect_images = all_images[incorrect_idx]
incorrect_labels = all_labels[incorrect_idx]

# For the 10 correct predictions (one from each class)
picked_correct = []
class_found = set()
for i in range(len(correct_images)):
    c = correct_labels[i].item()
    if c not in class_found:
        picked_correct.append(i)
        class_found.add(c)
    if len(class_found) == 10:
        break

if len(incorrect_images) > 30:
    picked_incorrect = np.random.choice(range(len(incorrect_images)), size=30, replace=False)
else:
    picked_incorrect = range(len(incorrect_images))

print("Showing CAMs for 10 correct predictions (one from each class):")
for idx in picked_correct:
    image = correct_images[idx]
    label = correct_labels[idx]
    pred_class, pred_prob, cam_pred, _ = get_single_image_cam(model, image, label, class_labels)
    show_result(image, label, pred_class, pred_prob, cam_pred, None, class_labels)

print("Showing CAMs for 30 incorrect predictions:")
count = 0
for idx in picked_incorrect:
    image = incorrect_images[idx]
    label = incorrect_labels[idx]
    pred_class, pred_prob, cam_pred, cam_gt = get_single_image_cam(model, image, label, class_labels)
    show_result(image, label, pred_class, pred_prob, cam_pred, cam_gt, class_labels)
    count += 1
    if count >= 30:
        break